{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance\n",
    "The Euclidean distance can be defined as the length of the line segment joining the two data\n",
    "points plotted on an n-dimensional Cartesian plane.\n",
    "\n",
    "More generally, consider two n-dimensional points (or vectors):\n",
    "* <b>v1</b>: $(q_1, q_2,...., q_n)$\n",
    "* <b>v2</b>: $(r_1, r_2,....., r_n)$\n",
    "\n",
    "Then, the Euclidean score is mathematically defined as:\n",
    "$$d(v_1, v_2) = \\sqrt{\\sum_{i=1}^n (q_i-r_i)^2}$$\n",
    "\n",
    "Euclidean scores can take any value between 0 and infinity. The lower the Euclidean score (or distance), the more similar the two vectors are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute Euclidean Distance.\n",
    "def euclidean(v1, v2):\n",
    "    #Convert 1-D Python lists to numpy vectors\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "\n",
    "    #Compute vector which is the element wise square of the difference\n",
    "    diff = np.power(np.array(v1)- np.array(v2), 2)\n",
    "\n",
    "    #Perform summation of the elements of the above vector\n",
    "    sigma_val = np.sum(diff)\n",
    "\n",
    "    #Compute square root and return final Euclidean score\n",
    "    euclid_score = np.sqrt(sigma_val)\n",
    "    \n",
    "    return euclid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define 3 users with ratings for 5 movies\n",
    "u1 = [5, 1, 2, 4, 5]\n",
    "u2 = [1, 5, 4, 2, 1]\n",
    "u3 = [5, 2, 2, 4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ratings, we can see that users 1 and 2 have extremely different tastes, whereas the tastes of users 1 and 3 are largely similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.483314773547883"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(u1, u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(u1, u3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users 1 and 3 have a much smaller Euclidean score between them than users 1 and 2. Therefore, in this case, the Euclidean distance was able to satisfactorily capture the relationships between our users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation\n",
    "\n",
    "Consider two users, Alice and Bob, who have rated the same five movies. Alice is extremely stingy with her ratings and never gives more than a 4 to any movie. On the other hand, Bob is more liberal and never gives anything below a 2 when rating movies. Let's define the matrices representing Alice and Bob and compute their Euclidean distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice = [1,1,3,2,4]\n",
    "bob = [2,2,4,3,5]\n",
    "\n",
    "euclidean(alice, bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a Euclidean distance of about 2.23. However, on closer inspection, we see that Bob always gives a rating that is one higher than Alice. Therefore, we can say that Alice and Bob's ratings are extremely correlated. In other words, if we know Alice's rating for a movie, we can compute Bob's rating for the same movie with high accuracy (in this case, by just adding 1).\n",
    "\n",
    "Consider another user, Eve, who has the polar opposite tastes to Alice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.324555320336759"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eve = [5,5,3,4,2]\n",
    "\n",
    "euclidean(eve, alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a very high score of 6.32, which indicates that the two people are very dissimilar. If we used Euclidean distances, we would not be able to do much beyond this. However, on inspection, we see that the sum of Alice's and Eve's ratings for a movie always add up to 6. Therefore, although very different people, one's rating can be used to accurately predict the corresponding rating of the other. Mathematically speaking, we say Alice's and Eve's ratings are strongly negatively correlated.\n",
    "\n",
    "Euclidean distances place emphasis on magnitude, and in the process, are not able to gauge the degree of similarity or dissimilarity well. This is where the Pearson correlation comes into the picture. The Pearson correlation is a score between -1 and 1, where -1 indicates total negative correlation (as in the case with Alice and Eve) and 1 indicates total positive correlation (as in the case with Alice and Bob), whereas 0 indicates that the two entities are in no way correlated with each other (or are independent of each other).\n",
    "\n",
    "Mathematically, the Pearson correlation is defined as follows:\n",
    "$$r=\\frac{\\sum_{i=1}^n (x_i-\\overline{x})(y_i-\\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\overline{x})^2} \\sqrt{\\sum_{i=1}^n (y_i-\\overline{y})^2}}$$\n",
    "Here, $\\overline{i}$ denotes the mean of all the elements in vector i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=1.0, pvalue=0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearsonr(alice, bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=-1.0, pvalue=0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(alice, eve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element of our list output is the Pearson score. We see that Alice and Bob have the highest possible similarity score, whereas Alice and Eve have the lowest possible score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=-1.0, pvalue=0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(bob, eve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "Mathematically, the Cosine similarity is defined as follows:\n",
    "$$cosine(x, y)=\\frac{x.y^T}{||x||.||y||}$$\n",
    "The cosine similarity score computes the cosine of the angle between two vectors in an ndimensional space. When the cosine score is 1 (or angle is 0), the vectors are exactly similar. On the other hand, a cosine score of -1 (or angle 180 degrees) denotes that the two vectors are exactly dissimilar to each other.\n",
    "\n",
    "Now, consider two vectors, x and y, both with zero mean. We see that when this is the case, the Pearson correlation score is exactly the same as the cosine similarity Score. In other words, for centered vectors with zero mean, the Pearson correlation is the cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
